{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7b1e020",
   "metadata": {},
   "source": [
    "# Results of first case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting import GroupNormalizer\n",
    "from pytorch_forecasting import TemporalFusionTransformer, QuantileLoss\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "import params\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b753d9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anwender\\AppData\\Local\\Temp\\ipykernel_23488\\2580056126.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  time_df[\"is_holiday_or_weekend\"] = time_df[\"is_holiday_or_weekend\"].replace(\n"
     ]
    }
   ],
   "source": [
    "time_df = pd.read_csv('dataframe.csv', index_col=0, sep=\";\", decimal=\".\")\n",
    "time_df = time_df.reset_index(drop=True)\n",
    "time_df[\"is_holiday_or_weekend\"] = time_df[\"is_holiday_or_weekend\"].replace(\n",
    "    {True: 1, False: 0}\n",
    ")\n",
    "pattern = r\"\\d{1,2}\\.?\\s*[A-Za-z]{3}\"\n",
    "time_df = time_df.replace(pattern, float(\"nan\"), regex=True)\n",
    "time_df = time_df.ffill(axis=0)\n",
    "time_df[\"Dayahead Prices Germany/Luxembourg\"] = time_df[\n",
    "    \"Dayahead Prices Germany/Luxembourg\"\n",
    "].astype(\"float\")\n",
    "\n",
    "training_cutoff = time_df[params.time_idx].max() - params.max_prediction_length\n",
    "training = TimeSeriesDataSet(\n",
    "    time_df[lambda x: x.hours_from_start <= training_cutoff],\n",
    "    time_idx=params.time_idx,\n",
    "    target=params.target,\n",
    "    group_ids=params.group_ids,\n",
    "    min_encoder_length=params.max_encoder_length // 2,\n",
    "    max_encoder_length=params.max_encoder_length,\n",
    "    min_prediction_length=params.min_prediction_length,\n",
    "    max_prediction_length=params.max_prediction_length,\n",
    "    static_categoricals=params.static_categoricals,\n",
    "    time_varying_known_reals=params.time_varying_known_reals,\n",
    "    time_varying_unknown_reals=params.time_varying_unknown_reals,\n",
    "    target_normalizer=GroupNormalizer(groups=[\"zone\"], transformation=\"softplus\"),\n",
    "    add_relative_time_idx=params.add_relative_time_idx,\n",
    "    add_target_scales=params.add_target_scales,\n",
    "    add_encoder_length=params.add_encoder_length,\n",
    "    allow_missing_timesteps=True,\n",
    ")\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training, time_df, predict=True, stop_randomization=True\n",
    ")\n",
    "\n",
    "# if you have a strong GPU, feel free to increase the number of workers\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True, batch_size=params.batch_size, num_workers=0\n",
    ")\n",
    "val_dataloader = validation.to_dataloader(\n",
    "    train=False, batch_size=params.batch_size * 10, num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7510b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example (you must use your actual model parameters!)\n",
    "best_tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=params.learning_rate,\n",
    "    hidden_size=params.hidden_size,\n",
    "    attention_head_size=params.attention_head_size,\n",
    "    dropout=params.dropout,\n",
    "    hidden_continuous_size=params.hidden_continuous_size,\n",
    "    output_size=params.output_size,  # there are 7 quantiles by default: [0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=params.log_interval,\n",
    "    reduce_on_plateau_patience=params.reduce_on_plateau_patience,\n",
    ")\n",
    "\n",
    "# Load state dict\n",
    "best_tft.load_state_dict(torch.load(\"best_tft_cpu_state_dict.pt\", map_location=\"cpu\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TST_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
